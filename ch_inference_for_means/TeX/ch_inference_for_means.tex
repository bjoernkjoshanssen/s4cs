\chapter{Inference for numerical data}
\label{inferenceForNumericalData}


%__________________
%\section{One-sample means with the $t$-distribution}
\section{One-sample means with the \texorpdfstring{$t$}{t}-distribution}
\label{oneSampleMeansWithTDistribution}

The main text's so-called ``Central Limit Theorem for normal data'' has the following more precise version:
The sampling distribution of the mean is normal when the sample observations are independent and come from a normal distribution. This is true for any sample size.
\index{Central Limit Theorem!normal data|)}



\subsection{Introducing the $t$-distribution}
\label{introducingTheTDistribution}

\index{t-distribution|(}
\index{distribution!$t$|(}

We define precisely what the $t$-distribution is here in the supplement.
It is simply the distribution of
\[
	T = (\overline X-\mu)/(S/\sqrt{n})
\]
where $S$ is the sample standard deviation given by
\[
	S^2 = \frac1{n-1}\sum_{i=1}^2 (X_i-\bar X)^2
\]
and $\overline X = \frac1n\sum_{i=1}^n X_i$, where $\{X_i\}_{i=1}^n$ are normal, identically distributed and independent.



The distribution of $T$ is called the $t$-distribution with $n-1$ degrees of freedom (df). Note that if $n=1$, we have $S=0$ with probability 1, so there is no such thing as the $t$-distribution with 0 degrees of freedom.

The pdfs get complicated at high df, but the $t$-distribution with 1 degree of freedom is also known as the Cauchy distribution and has pdf
\[
	f_T(t) = \frac1\pi \frac1{1+t^2}.
\]
\begin{exercise}
	Check that $\int_{-\infty}^\infty f_T(t)\,dt = 1$.\footnote{Recall that the derivative of $\arctan(t)$ is $1/(1+t^2)$.}
\end{exercise}
%\textC{\pagebreak}

We can think about this as follows. We write out the $t$ statistic in the $n=2$ case as:
\begin{eqnarray*}
	T=(\overline X-\mu)/(S/\sqrt{n}) &=& \frac{\frac12(X_1+X_2)-\mu}{\frac1{\sqrt n}\sqrt{ \frac1{2-1}\sum_{i=1}^2 (X_i-\bar X)^2}}\\
	&=&\frac{\frac12(X_1+X_2)-\mu}{\frac1{\sqrt 2}\sqrt{\sum_{i=1}^2 (X_i-\bar X)^2}}\\
	&=&\frac{\frac12(X_1+X_2)-\mu}{\frac1{\sqrt 2}\sqrt{2(\frac{X_1-X_2}2)^2}}\\
	&=& \frac{\frac12(X_1+X_2)-\mu}{\frac12 |X_1-X_2|}
\end{eqnarray*}
For simplicity, take $\mu=0$. Then
\[
	T^2 = \frac{(X_1+X_2)^2}{(X_1-X_2)^2}
\]
and $T^2\le t^2$ iff
\[
	(X_1+X_2)^2\le t^2(X_1-X_2)^2
\]
We would now have to perform a double integral over a region in $(X_1,X_2)$-plane, but we shall leave that for a course that requires multivariable calculus.

% {\color{red}
If we let $Z=X_1+X_2$ and $W=X_1-X_2$ then $Z$ and $W$ are both normal and their covariance is zero:
\[
\mathrm{Cov}(X_1+X_2,X_1-X_2)=\mathbb E((X_1+X_2)(X_1-X_2))=
\mathbb E(X_1^2)
-
\mathbb E(X_2^2)=0.
\]
It is a fact that jointly normal random variables with covariance zero are independent, and that $X_1+X_2$ and $X_1-X_2$ are jointly normal when
$X_1$ and $X_2$ are independent and normal. One definition of jointly normal is:
$X$ and $Y$ are jointly normal if $aX+bY$ has a normal distribution for all $a,b\in\mathbb R$.

Thus, $Z$ and $W$ are independent and we are looking at (when $\mu=0$) $T=Z/|W|$.
Then
\[
\frac{d}{dt}\mathbb P(T\le t) = \frac{d}{dt}\mathbb P(Z \le t |W|) = \frac{d}{dt}\int_{-\infty}^\infty \int_{-\infty}^{t|w|}f_Z(z)f_W(w)\,dz\,dw
\]
\[
=\int_{-\infty}^{\infty} |w| f_Z(t|w|)f_W(w)\, dw =\int_{-\infty}^{\infty} |w| \frac1{2\pi} e^{-(t|w|)^2/2} e^{-w^2/2}\, dw =
\]
Now use a $u$-substitution $u^2=(t^2+1)w^2$ and another one $v=u^2/2$ and get
\[
=\frac1{\pi}\int_0^{\infty} w e^{-(t^2+1)w^2/2}\,dw = \frac1{\pi} \int_0^{\infty} \frac{u e^{-u^2/2}}{t^2+1}\,du = \frac1{\pi(t^2+1)}\int_0^{\infty}e^{-v}\,dv
\]
\[
=\frac1{\pi(t^2+1)}.
\]
% }
%\subsection{One sample $t$-tests}
\label{oneSampleTTests}


\begin{termBox}{\tBoxTitle{When using a $t$-distribution, we use a T-score (same as Z-score)}
To help us remember to use the $t$-distribution, we use a $T$ to represent the test statistic, and we often call this a \term{T-score}. The Z-score and T-score represent how many standard errors the observed value is from the null value. In general we can say that a \emph{score} is an expression of the form $\frac{X-\E(X)}{\mathrm{SD}(X)}$; it will be a Z-score for a normally distributed random variable $X$ and a T-score for a random variable $X$ having a $t$-distribution.}
\end{termBox}


%__________________
\section{Paired data}
\label{pairedData}

\index{paired data|(}
\index{data!textbooks|(}


Mathematically, observations are paired if they are of the form $(X(\omega),Y(\omega))$ for the same outcome $\omega$. So for instance if $\omega$ is a textbook, $X(\omega)$ could be its price in the UCLA bookstore, and $Y(\omega)$ its price at Amazon:
%\begin{termBox}{\tBoxTitle{Paired data}
\begin{definition}
Two sets of observations $\mathcal A$ and $\mathcal B$ are paired if for each $a\in\mathcal A$ there is exactly one $\omega$ such that $a=X(\omega)$ and $Y(\omega)\in \mathcal B$, and vice versa.
\end{definition}
%}
%\end{termBox}

We can then consider the random variable $W(\omega) = X(\omega)-Y(\omega)$ in order to compare the means of the data in $\mathcal A$ and $\mathcal B$. For arbitrary sets $\mathcal A$ and $\mathcal B$ (not paired) this would not be possible. Note $X$ and $Y$ are not assumed independent, in fact as the UCLA/Amazon example indicates they typically will not be.

Thus, when drawing $n$ samples $\omega_1,\dots,\omega_n$ or in another expression $(X_1(\omega),\dots,X_n(\omega))$, we assume the $X_i$ are independent, as are the $Y_i$, but $X_i$ is not independent of $Y_i$.

When the data is not paired, we again have
\[
	(X_1(\omega),\dots,X_n(\omega))\text{ and }(Y_1(\omega'),\dots,Y_m(\omega'))
\]
where $n=m$ is allowed, but not required, and there is no implied relation between $X_1(\omega)$ and $Y_1(\omega')$ or in general between any particular index of the $X$'s and any particular index of the $Y$'s. Here $X_i(\omega)=X(\omega_i)$ where $\omega$ is really a vector of outcomes. In the paired case, $\omega=\omega'$. In both cases, when outcomes are distinct we assume they are selected independently.

%__________________
\section{Difference of two means}
\label{differenceOfTwoMeans}

The variance of a difference of (independent) means $\overline X-\overline Y$ is
\[
	\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}.
\]
So far this concerns the normal case. The $t$-distribution is used when the sample sizes are small enough that we cannot trust that the sample standard deviation is close enough to the actual standard deviation.
Thus we use the standard error,
\[
	\SE = \frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}.
\]

{

\subsection{Confidence interval for a difference of means}

\index{point estimate!difference of means|(}


\begin{termBox}{\tBoxTitle{Using the $t$-distribution for a difference in means}
\label{ConditionsForTwoSampleTDist}The $t$-distribution can be used for inference when working with the standardized difference of two means if (1) each sample meets the conditions for using the $t$-distribution and (2) the samples are independent.}
\end{termBox}

%\begin{termBox}{\tBoxTitle{Conditions for applying the $t$-distribution to $\bar{x}_1 - \bar{x}_2$}
%If the sample means, $\bar{x}_1$ and $\bar{x}_2$, each meet the criteria for using the $t$-distribution and the observations in the two samples are independent, then we can analyze the difference in sample means using the $t$-distribution.}
%\end{termBox}

\index{point estimate!difference of means|)}
\index{standard error (SE)!difference in means}

We can quantify the variability in the point estimate, $\bar{x}_{1} - \bar{x}_{2}$, using the following formula for its standard error:
\index{standard error (SE)!difference in means}
\begin{eqnarray*}
\SE_{\bar{x}_{1} - \bar{x}_{2}} = \sqrt{\frac{\sigma_{1}^2}{n_{1}} + \frac{\sigma_{2}^2}{n_{2}}}
\end{eqnarray*}
We usually estimate this standard error using standard deviation estimates  based on the samples:
\begin{align*}
\SE_{\bar{x}_{1} - \bar{x}_{2}}
	&= \sqrt{\frac{\sigma_{1}^2}{n_{1}} + \frac{\sigma_{2}^2}{n_{2}}} \\
	&\approx \sqrt{\frac{s_{1}^2}{n_{1}} + \frac{s_{2}^2}{n_{2}}}
	%= \sqrt{\frac{5.17^2}{9} + \frac{2.76^2}{9}} = 1.95
\end{align*}
Because we will use the $t$-distribution, we also must identify the appropriate degrees of freedom. This can be done using computer software. An alternative technique is to use the smaller of $n_1 - 1$ and $n_2 - 1$, which is the method we will typically apply in the examples and guided practice. This technique for degrees of freedom is conservative with respect to a Type~1 Error; it is more difficult to reject the null hypothesis using this df method.

%\newpage
\subsection{The Welch $t$-test}
%To explain this more fully here in \emph{Statistics for Calculus Students},
Welch discovered that in the case of unequal variances, it is approximately correct to use a $t$-distribution with a peculiar count for degrees of freedom.

The search for an exact test is ongoing and is called the \emph{Behrens-Fisher problem}. The problem is that, whereas Gosset (Student) showed that $(\overline X-\mu)/S$ has a distribution that does not depend on the unknown $\sigma$, the analogous quantity in the 2-sample case $(\overline X_1-\overline X_2-0)/\sqrt{S^2_1/n_1+S^2_2/n_2}$ has not that property. So we would want some function $f(\overline X_1,\overline X_2,S_1,S_2)$ which used the information given in a powerful way, and had a known distribution under the null hypothesis.

Welch's number of degrees of freedom (which may not be an integer) is
\[
	\nu :=
	\frac{
		\left(
			 \frac{s_1^2}{n_1}
			+\frac{s_2^2}{n_2}
		\right)^2
	}{
			 \frac{s_1^4}{n_1^2\nu_1}
			+\frac{s_2^4}{n_2^2\nu_2}
	}
\]
where $\nu_i=n_i-1$. The claim is now that the quantity $\mu:=\min(\nu_1,\nu_2)$ is a conservative count compared to $\nu$. In other words, we are more likely to reject our null hypothesis with $\nu$. That is, for a given value $t$, the probability of something as extreme as $t$ is lower (hence more likely to be below $\alpha=5\%$ say) according to the $t(\nu)$ distribution than according to the $t(\mu)$ distribution. In other words, $t(\nu)$ is closer to the normal distribution, which has very low probability of given extreme values $t$ (very thin tails). In other words yet, $\mu\le\nu$. Let us check that. This means
\[
	\frac{\left(
			\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}
		\right)^2}{
			\frac{s_1^4}{n_1^2\nu_1}+\frac{s_2^4}{n_2^2\nu_2}
		}
		\ge \min(\nu_1,\nu_2).
\]
Let us assume $\nu_1\le \nu_2$. Then it suffices to show
\[
	\frac{\left(
			\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}
		\right)^2}{
			\frac{s_1^4}{n_1^2\nu_1}+\frac{s_2^4}{n_2^2\nu_2}
		}
		\ge \nu_1
\]
\[
	\left(
		\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}
	\right)^2
	\ge
		\left(\frac{s_1^4}{n_1^2\nu_1}+\frac{s_2^4}{n_2^2\nu_2}\right)
	\nu_1.
\]
And indeed,
\[
	\left(
		\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}
	\right)^2
	\ge
	\frac{s_1^4}{n_1^2}+\frac{s_2^4}{n_2^2}
	\ge
	\frac{s_1^4}{n_1^2}+\frac{s_2^4\nu_1}{n_2^2\nu_2}
	=
		\left(\frac{s_1^4}{n_1^2\nu_1}+\frac{s_2^4}{n_2^2\nu_2}\right)
	\nu_1.
\]
We see that when $n_1=n_2=2$, the difference between the Welch df and the conservative df is at most 1, basically since $2xy\le x^2+y^2$.

\begin{termBox}{\tBoxTitle{Distribution of a difference of sample means}
The sample difference of two means, $\bar{x}_1 - \bar{x}_2$, can be modeled using the $t$-distribution and the standard error
\begin{eqnarray}
\textstyle
\SE_{\bar{x}_{1} - \bar{x}_{2}} = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}
\label{seOfDifferenceInMeans}
\end{eqnarray}
when each sample mean can itself be modeled using a $t$-distribution and the samples are independent. To calculate the degrees of freedom, use statistical software or the smaller of $n_1 - 1$ and $n_2 - 1$.}
\end{termBox}
%\textC{\vspace{-10mm}}


\subsection{Hypothesis tests based on a difference in means}

We use $\min(n_1-1,n_2-1)$ as our \emph{df}; this is conservative in the sense that it makes it harder to reject a null hypothesis. We can discuss the relationship with Welch's test.

We need the individual distributions of $n_1$ and $n_2$ points to be normal, in theory, in order to apply the $t$-test.
%The Google Sheets code for $t$-tests:

%\textC{\newpage}

The critical value for the $t$-test is $t^{\star}_{\text{df}}$. % or $t^*_{\text{df}}$.



\subsection{Examining the standard error formula} %(special topic)

The formula for the standard error of the difference in two means is similar to the formula for other standard errors. Recall that the standard error of a single mean, $\bar{x}_1$, can be approximated by\textC{\vspace{-3mm}}
\begin{align*}
\SE_{\bar{x}_1} = \frac{s_1}{\ \sqrt{n_1}\ }
\end{align*}
where $s_1$ and $n_1$ represent the sample standard deviation and sample size.

The standard error of the difference of two sample means can be constructed from the standard errors of the separate sample means:
\begin{eqnarray}
\SE_{\bar{x}_{1} - \bar{x}_{2}}
	= \sqrt{\SE_{\bar{x}_1}^2 + \SE_{\bar{x}_2}^2}
	= \sqrt{\frac{s_1^2}{{n_1}} + \frac{s_2^2}{{n_2}}}
\label{seOfDiffOfMeansInTermsOfSEOfIndividualMeans}
\end{eqnarray}
This special relationship follows from probability theory.

\begin{exercise}\label{derivingSEForDiffOfTwoMeansExercise}
	Prerequisite: Section~\ref{randomVariablesSection}.
	We can rewrite Equation~(\ref{seOfDiffOfMeansInTermsOfSEOfIndividualMeans}) in a different way:
	\begin{eqnarray*}
	\SE_{\bar{x}_{1} - \bar{x}_{2}}^2 = \SE_{\bar{x}_1}^2 + \SE_{\bar{x}_2}^2
	\end{eqnarray*}
	Explain where this formula comes from using the ideas of probability theory.\footnote{The standard error squared represents the variance of the estimate. If $X$ and $Y$ are two random variables with variances $\sigma_x^2$ and $\sigma_y^2$, then the variance of $X-Y$ is $\sigma_x^2 + \sigma_y^2$. Likewise, the variance corresponding to $\bar{x}_1 - \bar{x}_2$ is $\sigma_{\bar{x}_1}^2 + \sigma_{\bar{x}_2}^2$. Because $\sigma_{\bar{x}_1}^2$ and $\sigma_{\bar{x}_2}^2$ are just another way of writing $\SE_{\bar{x}_1}^2$ and  $\SE_{\bar{x}_2}^2$, the variance associated with $\bar{x}_1 - \bar{x}_2$ may be written as $\SE_{\bar{x}_1}^2 + \SE_{\bar{x}_2}^2$.}
\end{exercise}


\subsection{Pooled standard deviation estimate}
\label{pooledStandardDeviations}

Occasionally, two populations will have standard deviations that are so similar that they can be treated as identical. For example, historical data or a well-understood biological mechanism may justify this strong assumption. In such cases, we can make the $t$-distribution approach slightly more precise by using a pooled standard deviation.

The \term{pooled standard deviation} of two groups is a way to use data from both samples to better estimate the standard deviation and standard error. If $s_1^{}$ and $s_2^{}$ are the standard deviations of groups 1 and 2 and there are good reasons to believe that the population standard deviations are equal, then we can obtain an improved estimate of the group variances by pooling their data:
\begin{align*}
s_{pooled}^2 = \frac{s_1^2\times (n_1-1) + s_2^2\times (n_2-1)}{n_1 + n_2 - 2}
\end{align*}
where $n_1$ and $n_2$ are the sample sizes, as before. To use this new statistic, we substitute $s_{pooled}^2$ in place of $s_1^2$ and $s_2^2$ in the standard error formula, and we use an updated formula for the degrees of freedom:
\begin{align*}
\text{df} = n_1 + n_2 - 2
\end{align*}

The benefits of pooling the standard deviation are realized through obtaining a better estimate of the standard deviation for each group and using a larger df parameter for the $t$-distribution. Both of these changes may permit a more accurate model of the sampling distribution of $\bar{x}_1 - \bar{x}_2$, if the standard deviations of the two groups are~equal.

\begin{caution}
{Pool standard deviations only after careful consideration}
{A pooled standard deviation is only appropriate when background research indicates the population standard deviations are nearly equal. When the sample size is large and the condition may be adequately checked with data, the benefits of pooling the standard deviations greatly diminishes.}
\end{caution}
}

%\subsection{Pooled $t$-test}
There are several flavors of $t$-test: one-sample, two-sample, paired, and \emph{pooled}.
The \emph{pooled} flavor is used when, on the one hand, the sample sizes are small enough that we do not have a good idea what the standard deviations are; but on the other hand, we have reason, somehow, to believe that the standard deviations of the two groups should come out to be the same.
Now where does the formula
\[
	s^2_{\text{pooled}} = \frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}\tag{*}
\]
and degrees of freedom $n_1+n_2-2$, come from?
One answer is that it is an \term{unbiased estimator} of $\sigma^2$, in other words, its expectation is $\sigma^2$.
\[
	\E(s^2_{\text{pooled}}) = \frac{(n_1-1)\E(s_1^2)+(n_2-1)\E(s_2^2)}{n_1+n_2-2} = \frac{(n_1-1)\sigma^2+(n_2-1)\sigma^2}{n_1+n_2-2} = \sigma^2.
\]
But, we could have just used $s_1^2$, or say $(s_1^2+s_2^2)/2$, if that were our only goal.\footnote{%Lara
You may wonder whether it would make sense to use a weighted average of the standard deviations $s_1$, $s_2$ in place of the squared root of a weighted average of $s_1^2$ and $s_2^2$. However, the expectation of such a quantity will not be what we want, as $\E(s_1)$ is a more complicated thing that $\E(s_1^2)$.}
Notice that if $n_2$ is much greater than $n_1$ then ($*$) is close to $s_2^2$.
So among all statistics of the form $\alpha s_1^2+\beta s_2^2$, where $\alpha+\beta=1$, perhaps $s^2_{\text{pooled}}$ minimizes
\begin{eqnarray*}
	  \E((\alpha s_1^2+\beta s_2^2-\sigma^2)^2)&=& \E(\alpha^2s_1^4+\beta^2s_2^4+\sigma^4+2\alpha\beta s_1^2s_2^2 -2\alpha s_1^2\sigma^2-2\beta s_2^2\sigma^2)\\
	&=& \alpha^2 f(n_1) + \beta^2 f(n_2) + \sigma^4 + 2\alpha\beta\sigma^4 - 2\alpha\sigma^4-2\beta\sigma^4\\
	&=& \alpha^2 f(n_1) + \beta^2 f(n_2) + (1 + 2\alpha\beta - 2\alpha-2\beta)\sigma^4\\
	&=& \alpha^2 f(n_1) + \beta^2 f(n_2) + (2\alpha\beta - 1)\sigma^4
\end{eqnarray*}
where\footnote{Source: Wolfram MathWorld \url{http://mathworld.wolfram.com/SampleVarianceDistribution.html}, line 23.}
\begin{eqnarray*}
	f(n)=\E(s^4)&=&\left[\frac{n}{n-1}\right]^2\frac{n-1}{n^3}((n-1)\mu_4+(n^2-n+3)\mu_2^2)\\
	&=& \frac{1}{n(n-1)}((n-1)\mu_4+(n^2-n+3)\mu_2^2)
\end{eqnarray*}
for sample size $n$ where $\mu_n$ is the $n$th central moment. For normal distributions $\mu_4=3\sigma^4$ and $\mu_2=\sigma^2$ so
\[
	f(n) = \frac{1}{n(n-1)}(3(n-1)+n^2-n+3)\sigma^4 = \frac{n+2}{n-1}\sigma^4
\]
So we consider
\[
	g(\alpha) = \alpha^2\frac{n_1+2}{n_1-1} + (1-\alpha)^2 \frac{n_2+2}{n_2-1} + 2\alpha(1-\alpha) - 1
\]
and seek to minimize it.
Let use write $=^!$ for ``we would like to prove that'', or more modestly, ``we would like to investigate whether''.
We have:
\begin{eqnarray*}
	0 &=^!& g'(\alpha) = 2\alpha\frac{n_1+2}{n_1-1} - 2(1-\alpha) \frac{n_2+2}{n_2-1} + 2(1-2\alpha)\\
	0 &=^!& \alpha\frac{n_1+2}{n_1-1} - (1-\alpha) \frac{n_2+2}{n_2-1} + (1-2\alpha)\\
	0 &=^!& \alpha\frac{3}{n_1-1} - (1-\alpha) \frac{3}{n_2-1}\\
	0 &=^!& \alpha\frac{1}{n_1-1} - (1-\alpha) \frac{1}{n_2-1}\\
	\alpha &=^!& \frac{n_1-1}{n_1-1+n_2-1}
\end{eqnarray*}
So it is indeed $\frac{n_1-1}{n_1+n_2-2}$.


\begin{example}{Learning assistants.}%from Celine et al. Module 4
	The University of Hawai\textquoteleft i at M\=anoa Department of Mathematics used \emph{learning assistants}
	in MATH 203 (Calculus for Business and Social Sciences) in Spring 2017 and Fall 2017.
	Prior to that, data is available for the Fall 2008 semester, the Spring 2009 semester, the Fall 2009 semester, and so on until and including Fall 2016.
	It was found that the mean grade point (where A is 4.0, A- is 3.7, B+ is 3.3, B is 3.0, and so on)
	for the period before learning assistants was 2.29 with a standard error (standard deviation for the means) of 0.098.
	The mean for Spring 2017 and Fall 2017 was 2.385. This is $\frac{2.385-2.29}{0.098}$ standard errors above the null value.
	However, since we are using two semesters for 2017 and not just one, we need to compare to $0.098/\sqrt{2}$ instead of $0.098$.
	This still doesn't make the impact of learning assistants on mean grades statistically significant, but it makes it closer to significance.
\end{example}